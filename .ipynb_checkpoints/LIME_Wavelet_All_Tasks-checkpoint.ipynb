{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "horizontal-feelings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc, rcParams\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "import ipywidgets as widgets\n",
    "from scipy.fftpack import fft\n",
    "from scipy.fftpack import ifft\n",
    "from scipy.signal import welch\n",
    "from detecta import detect_peaks\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "# from xgboost import XGBClassifier\n",
    "import dill\n",
    "import pywt\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "# import pdfkit\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-virus",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "demanding-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(accDir, annotFile):\n",
    "    files = os.listdir(accDir)\n",
    "    files_csv = [f for f in files if f[-3:] == 'csv']\n",
    "    empatica_dict = dict()\n",
    "    for f in files_csv:\n",
    "        data = np.genfromtxt(accDir+f, delimiter=',') # creates numpy array for each Empatica acc csv file\n",
    "        key = int(float(f.strip(\"ACC.csv\")))\n",
    "        empatica_dict[key] = data\n",
    "    tmp = pd.read_excel(annotFile, sheet_name=None, usecols='A:B')\n",
    "    annot_dict = dict(zip(tmp.keys(), [i.dropna() for i in tmp.values()])) # Remove the rows with NaN values (some with ladder 2 missing)\n",
    "    return empatica_dict, annot_dict\n",
    "\n",
    "def getLabeledDict(empatica_dict, annot_dict, subject_ids):\n",
    "    labeled_dict = {}; taskInd_dict = {}\n",
    "    for id in subject_ids:\n",
    "        start_time = int(empatica_dict[id][0,0])\n",
    "        acc = empatica_dict[id][2:,:]\n",
    "        label = list(map(lambda i: i.replace(\"_end\", \"\").replace(\"_start\", \"\"), annot_dict['P'+ str(id)].taskName.tolist()))\n",
    "        task_time= list(map(lambda i: time.mktime(datetime.datetime.strptime(i[:6] + '20' + i[6:], \"%m/%d/%Y %H:%M:%S\").timetuple()),\n",
    "                            annot_dict['P'+ str(id)].startTime_global.tolist()))\n",
    "        task_ind = [int(x - start_time)*SR for x in task_time]\n",
    "        taskInd_dict[id] = task_ind\n",
    "        label_tmp = np.empty(acc.shape[0], dtype=object)\n",
    "        for i, (j, k) in enumerate(zip(task_ind[0::2], task_ind[1::2])):\n",
    "            tmpInd = 2*i\n",
    "            label_tmp[j:k] = label[tmpInd]\n",
    "        labeled_dict[id] = np.hstack((acc, label_tmp.reshape(label_tmp.shape[0],1)))\n",
    "    return labeled_dict, taskInd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adjustable-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "sepAccDict, sepAnnotDict = readData(accDir='Data/Acc Data/separate/', annotFile='Data/Annotation Data/separate.xlsx')\n",
    "SR=int(sepAccDict[8][1,0])\n",
    "\n",
    "sepSubIDs = list(range(8,45))\n",
    "sepLabeledDict, sepTaskIndDict = getLabeledDict(sepAccDict, sepAnnotDict, sepSubIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forbidden-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArrDict(labeledDict, subject_ids, windowLen):\n",
    "    ArrDict = {}\n",
    "    for id in subject_ids:\n",
    "        df = pd.DataFrame(data=labeledDict[id], columns=['aX', 'aY', 'aZ', 'label'])\n",
    "        grDict = dict(tuple(df.groupby(by='label'))) # dict values are pd DF's\n",
    "        sigArr = np.zeros((0, windowLen, 4))\n",
    "        for key in grDict.keys():\n",
    "            task_df = grDict[key]\n",
    "            tmp1 = [task_df.iloc[i:i+windowLen, :] for i in range(0, task_df.shape[0], windowLen)]\n",
    "            if len(tmp1[-1]) != windowLen: del tmp1[-1]\n",
    "            sigArr = np.vstack((sigArr, np.array(tmp1)))\n",
    "        ArrDict[id] = sigArr\n",
    "    return ArrDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "divided-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyFilter(sig, SR, n, fc):\n",
    "    w = fc/(SR/2)\n",
    "    b, a = signal.butter(n, w, 'low')\n",
    "    filteredSig = signal.filtfilt(b, a, sig)\n",
    "    return filteredSig\n",
    "\n",
    "def fftCalc(sig, sampRate, windowLen):\n",
    "    f_values = np.linspace(0, sampRate/2, windowLen//2)\n",
    "    fft_values = 2/windowLen * np.abs(fft(sig)[0:windowLen//2])\n",
    "    filtered_f = f_values[f_values<10]\n",
    "    filtered_fft = fft_values[f_values<10]\n",
    "    return filtered_f, filtered_fft\n",
    "\n",
    "def fftPeaks(sig, filtered_f, filtered_fft, percentile, denominator, sampRate, no_peaks):\n",
    "    signal_min = np.percentile(sig, percentile)\n",
    "    signal_max = np.percentile(sig, 100-percentile)\n",
    "    mph = (signal_max - signal_min)/denominator # minimum peak height\n",
    "    indices_peaks = detect_peaks(filtered_fft, mph=mph)\n",
    "    if len(indices_peaks) >= no_peaks:\n",
    "        peakFeatures = list(filtered_f[indices_peaks])[:no_peaks] + list(filtered_fft[indices_peaks])[:no_peaks]\n",
    "    else:\n",
    "        missing = no_peaks-len(indices_peaks)\n",
    "        peakFeatures = list(filtered_f[indices_peaks])+[0]*missing + list(filtered_fft[indices_peaks])+[0]*missing\n",
    "    return peakFeatures\n",
    "\n",
    "def getStats(sig):\n",
    "    mean_ = np.mean(sig)\n",
    "    rms_ = np.sqrt(np.mean(np.square(sig)))\n",
    "    mad_ = np.mean(np.absolute(sig-np.mean(sig)))\n",
    "    std_ = np.std(sig)\n",
    "    min_ = min(sig)\n",
    "    max_ = max(sig)\n",
    "    med_ = np.percentile(sig, 50)\n",
    "    perc25 = np.percentile(sig, 25)\n",
    "    perc75 = np.percentile(sig, 75)\n",
    "    stats_ = [mean_, rms_, mad_, std_, min_, max_, med_, perc25, perc75]\n",
    "    return stats_\n",
    "\n",
    "def getCrossings(sig):\n",
    "    zeroCross = np.count_nonzero(np.diff(np.sign(sig)))\n",
    "    centeredSig = sig - np.mean(sig)\n",
    "    meanCross = np.count_nonzero(np.diff(np.sign(centeredSig)))\n",
    "    return [zeroCross, meanCross]\n",
    "\n",
    "def getEntropy(sig):\n",
    "    count = Counter(sig).most_common()\n",
    "    probability = [elem[1]/len(sig) for elem in count]\n",
    "    return [stats.entropy(probability)]\n",
    "\n",
    "def extract_features_dicts(dataset, key, windowLen, sampRate, denominator, no_peaks, waveletName, featureSet, percentile=5):\n",
    "    list_of_features = []\n",
    "    for samp_no in range(0, dataset.shape[0]):\n",
    "        features = []\n",
    "        for sig_comp in range(0, dataset.shape[2] - 1):\n",
    "            signal = dataset[samp_no, :, sig_comp]\n",
    "            if featureSet == 'Time':\n",
    "                features += getStats(applyFilter(signal, SR=sampRate, n=4, fc=10))\n",
    "            elif featureSet == 'FrequencyAll':\n",
    "                filtered_f, filtered_fft = fftCalc(signal, sampRate, windowLen)\n",
    "                features += list(filtered_fft)\n",
    "            elif featureSet == 'FrequencyPeaks':\n",
    "                filtered_f, filtered_fft = fftCalc(signal, sampRate, windowLen)\n",
    "                features += fftPeaks(signal, filtered_f, filtered_fft, percentile, denominator, sampRate, no_peaks)\n",
    "            elif featureSet == 'TimeFrequency':\n",
    "                waveletCoeffs = pywt.wavedec(signal, waveletName)\n",
    "                for coeff in waveletCoeffs:\n",
    "                    features += getStats(coeff) + getEntropy(coeff) + getCrossings(coeff)\n",
    "            \n",
    "        modifiedLabel = ''.join([i for i in dataset[samp_no, 0, 3] if not i.isdigit()]) # remove any numbers from label\n",
    "        features.append(modifiedLabel) # Add modified label as last column\n",
    "        features.append(key)\n",
    "        list_of_features.append(features)\n",
    "    features_df = pd.DataFrame(np.array(list_of_features), columns=list(range(1, len(list_of_features[0]) -1)) + ['label', 'subID'])\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-cycling",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acceptable-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTrainTestData(featureDict, trainSub, testSub):\n",
    "    test_df = pd.concat([featureDict[x] for x in testSub]).reset_index().drop(['index'], axis=1)\n",
    "    X_test = test_df.loc[:, test_df.columns != 'label']\n",
    "    Y_test = test_df.label\n",
    "    trainDict = {}\n",
    "    for key in trainSub:\n",
    "        trainDict[key] = featureDict[key]\n",
    "        trainDict[key]['subID'] = key\n",
    "    train_df = pd.concat([trainDict[x] for x in trainDict.keys()]).reset_index().drop(['index'], axis=1)\n",
    "    X_train = train_df.drop(['label', 'subID'], axis=1)\n",
    "    Y_train = train_df.label\n",
    "    return train_df, X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "encouraging-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTestTrainSubjects(sub_list, fold, num_of_folds):\n",
    "    random.seed(1365)\n",
    "    random.shuffle(sub_list)\n",
    "    test_subs = sub_list[fold-1::num_of_folds]\n",
    "    train_subs = list(set(sub_list) - set(test_subs))\n",
    "    return test_subs, train_subs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-modern",
   "metadata": {},
   "source": [
    "# Get Time-Frequency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "empirical-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowLen = 320 # 320 or 128\n",
    "sepArrDict = getArrDict(sepLabeledDict, sepSubIDs, windowLen)\n",
    "feature = 'TimeFrequency'# 'FrequencyAll' or 'TimeFrequency'\n",
    "\n",
    "sub_list = list(sepArrDict.keys())\n",
    "test_subs, train_subs = GetTestTrainSubjects(sub_list, fold=1, num_of_folds=5)\n",
    "\n",
    "features_df = {}\n",
    "for key in sepArrDict.keys():\n",
    "    features_df[key] = extract_features_dicts(dataset=sepArrDict[key], key=key, windowLen=windowLen, sampRate=SR, denominator=15,\n",
    "                                              no_peaks=5, waveletName='db4', featureSet=feature, percentile=5)\n",
    "train_df, X_train_df, Y_train, X_test_df, Y_test = GetTrainTestData(features_df, train_subs, test_subs)\n",
    "X_train = StandardScaler().fit_transform(X_train_df)\n",
    "X_test = StandardScaler().fit_transform(X_test_df.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-egyptian",
   "metadata": {},
   "source": [
    "## Import Fold 1,  *k*-NN, 10 Seconds windows, Frequency and Time-Frequency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "reliable-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('./outputs/classification/inter-subject/Fold1_FrequencyAll_KNN_320.pickle', 'rb')\n",
    "Frequency_results = pickle.load(infile)\n",
    "infile.close()\n",
    "clf_best_Freq = Frequency_results['clf_best']\n",
    "Y_pred_Freq = Frequency_results['TruePredY'].Predicted\n",
    "\n",
    "infile = open('./outputs/classification/inter-subject/Fold1_TimeFrequency_KNN_320.pickle', 'rb')\n",
    "Time_Frequency_results = pickle.load(infile)\n",
    "infile.close()\n",
    "clf_best_Time_Freq = Time_Frequency_results['clf_best']\n",
    "Y_pred_Time_Freq = Time_Frequency_results['TruePredY'].Predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-ordinance",
   "metadata": {},
   "source": [
    "### Set Names for Features (Time-Freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "addressed-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureNames = []\n",
    "for i in ['X', 'Y', 'Z']:\n",
    "    for j in ['Approx-5', 'Deatil-5', 'Detail-4', 'Detail-3', 'Detail-2', 'Detail-1']:\n",
    "        for k in ['Mean', 'RMS', 'MAD', 'STD', 'Min', 'Max', 'Median', '25-Perc', '75-Perc', 'Entropy',\n",
    "                  '0-Cross', 'Mean-Cross']:\n",
    "            featureNames.append(k + ' ' + j + ' ' + i)\n",
    "\n",
    "X_test_modified = X_test_df.copy()\n",
    "X_test_modified['label'] = Y_test\n",
    "X_test_modified['Freq_predicted'] = Y_pred_Freq\n",
    "X_test_modified['Time_Freq_predicted'] = Y_pred_Time_Freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-colon",
   "metadata": {},
   "source": [
    "## LIME for all Activities\n",
    "\n",
    "> **Note:** The following code snippet takes around `7 hours` to run. Its output is saved in \"outputs/lime/Wavelet_allActivities\" directory and the following code snippets read the files from there to create the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "spatial-insight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time = 22774 seconds\n"
     ]
    }
   ],
   "source": [
    "# start_time = time.time()\n",
    "# for label_ in X_test_modified.label.unique():\n",
    "#     Idx_for_exp = np.where(X_test_modified.label.unique()==label_)[0][0]\n",
    "#     Idxs = X_test_modified[(X_test_modified.label==label_) & (X_test_modified.Freq_predicted==label_) & (X_test_modified.Time_Freq_predicted==label_)].index.values\n",
    "#     random.seed(2021)\n",
    "#     selected_obseravtions_panel = random.sample(set(Idxs), 30)\n",
    "#     selected_features_proba_panel = []\n",
    "#     for samp_Idx in selected_obseravtions_panel:\n",
    "#         observation_features = X_test[samp_Idx,:]\n",
    "#         explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=featureNames, class_names=np.unique(train_df.label),\n",
    "#                                                            discretize_continuous=True, random_state=1365)\n",
    "#         exp = explainer.explain_instance(observation_features, clf_best_Time_Freq.predict_proba, num_features=30 , top_labels=1)\n",
    "#         selected_features_proba_panel.extend(exp.as_map()[Idx_for_exp])\n",
    "        \n",
    "#     with open('./outputs/lime/Wavelet_allActivities/LIME_Wavelet_' + label_ + '.pickle', 'wb') as outfile:\n",
    "#         pickle.dump(dict(selected_features_proba_panel=selected_features_proba_panel,\n",
    "#                          selected_obseravtions_panel=selected_obseravtions_panel), outfile)\n",
    "\n",
    "# print('Elapsed Time = {:0.0f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-hurricane",
   "metadata": {},
   "source": [
    "### Create Probability Bar Graphs for the 30 Most Frequently Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "developing-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_dict = {}\n",
    "for label_ in X_test_modified.label.unique():\n",
    "    with open('./outputs/lime/Wavelet_allActivities/LIME_Wavelet_' + label_ + '.pickle', 'rb') as infile:\n",
    "        pickle_dict_panel = pickle.load(infile)\n",
    "\n",
    "    selected_features_proba_panel = pickle_dict_panel['selected_features_proba_panel']\n",
    "    selected_obseravtions_panel = pickle_dict_panel['selected_obseravtions_panel']\n",
    "    \n",
    "    ############# Find indices of the 30 most frequently selected features\n",
    "    features_ = [elem[0] for elem in selected_features_proba_panel]\n",
    "    most_frequent_ = Counter(features_).most_common(30)\n",
    "    LIME_selected_feature_Idxs = [elem[0] for elem in most_frequent_]\n",
    "\n",
    "    ############# Create dictionary with unique features as key and the list of its probabilities\n",
    "    res = defaultdict(list)\n",
    "    for i,j in selected_features_proba_panel:\n",
    "        res[i].append(j)\n",
    "    feature_proba_dict_ = dict(res)\n",
    "\n",
    "    ############ Use indices of most frequent features to select\n",
    "    probabilities_ = [feature_proba_dict_[key] for key in LIME_selected_feature_Idxs]\n",
    "    feature_proba_dict = dict(zip(LIME_selected_feature_Idxs, probabilities_))\n",
    "\n",
    "    ############# Create dictionary with feature name as key and mean of probabilities as values; then sort based on dict absolute values\n",
    "    mean_probabilities = list(map(lambda x: np.mean(x), list(feature_proba_dict.values())))\n",
    "    feature_meanProba_dict = dict(zip(np.array(featureNames)[LIME_selected_feature_Idxs], mean_probabilities))\n",
    "    sorted_list_ofTuples = sorted(feature_meanProba_dict.items(), key=lambda i: abs(i[1]))\n",
    "    sorted_dict = dict(sorted_list_ofTuples)\n",
    "    \n",
    "    lime_dict[label_] = sorted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-breath",
   "metadata": {},
   "source": [
    "# Visualize LIME Output for all Activities in Tabs\n",
    "\n",
    "> **Note:** The plots from the following codesnippet are created in tabsets and thus not visible in the notebook. You can see them in the output html under \"outputs/htmls\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "backed-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc('text', usetex=True)\n",
    "rc('font', weight='bold')\n",
    "rcParams['text.latex.preamble'] = [r'\\usepackage{sfmath} \\boldmath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cardiovascular-regulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72108595cf994a0bae625abf4e3a48cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_tabs = list(lime_dict.keys())\n",
    "sub_tab=[widgets.Output() for i in range(len(labels_tabs))]\n",
    "tab = widgets.Tab(sub_tab)\n",
    "for i, act in enumerate(labels_tabs):\n",
    "    tab.set_title(i, act)\n",
    "    sorted_dict = lime_dict[act]\n",
    "    with sub_tab[i]:\n",
    "        y_pos = np.arange(len(sorted_dict))\n",
    "        f, ax = plt.subplots(1,figsize=(19,10))\n",
    "        ax.barh(y_pos, sorted_dict.values(), color='maroon')\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(sorted_dict.keys(), fontweight='bold')\n",
    "        ax.set_xlim([-0.05, 0.205])\n",
    "        ax.set_ylim([-1, 32])\n",
    "        ax.text(0.025, 30.5, r'\\textbf{' + act + '}', fontsize=20, c='mediumblue')\n",
    "        ax.text(-0.04, 30.5, r'\\textbf{Not ' + act + '}', fontsize=20, c='mediumblue')\n",
    "        ax.set_title(r'\\textbf{Probability of the 30 Most Frequently Selected Features}', fontsize=20)\n",
    "        ax.set_xlabel(r'\\textbf{Probability}', fontsize=20)\n",
    "        ax.set_ylabel(r'\\textbf{Most Frequently Selectd Features}', fontsize=20)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        f.tight_layout()\n",
    "        f.savefig('./outputs/lime/Wavelet_allActivities/' + act + '_Wavelet_proba.png')\n",
    "        plt.show()\n",
    "\n",
    "display(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-construction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-nutrition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
